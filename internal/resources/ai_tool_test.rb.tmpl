#!/usr/bin/env ruby
# frozen_string_literal: true

require "yaml"
require "json"
require "active_support/core_ext/object/blank"

WORKSPACE_ROOT = ENV["DV_AI_TOOL_ROOT"] || File.expand_path("..", __dir__)
CONFIG_PATH = File.join(WORKSPACE_ROOT, "tool.yml")
SCRIPT_PATH = File.join(WORKSPACE_ROOT, "script.js")
DEFAULT_PAYLOAD_PATH = File.join(WORKSPACE_ROOT, "test_payload.json")
PAYLOAD_PATH = ARGV[0] ? File.expand_path(ARGV[0], WORKSPACE_ROOT) : DEFAULT_PAYLOAD_PATH

def abort_with(message)
  STDERR.puts("[ai-tool test] #{message}")
  exit(1)
end

abort_with("Missing #{CONFIG_PATH}") unless File.exist?(CONFIG_PATH)
abort_with("Missing #{SCRIPT_PATH}") unless File.exist?(SCRIPT_PATH)

def load_yaml(path)
  YAML.safe_load(File.read(path), permitted_classes: [Date], aliases: true) || {}
rescue Psych::SyntaxError => e
  abort_with("Invalid YAML in #{path}: #{e.message}")
end

config = load_yaml(CONFIG_PATH)
script_source = File.read(SCRIPT_PATH)
payload_data =
  if File.exist?(PAYLOAD_PATH)
    JSON.parse(File.read(PAYLOAD_PATH))
  else
    {}
  end

name = config["name"].to_s.strip
tool_name = config["tool_name"].to_s.strip
description = config["description"].to_s.strip
summary = config["summary"].to_s.strip

abort_with("Set 'name' inside tool.yml") if name.blank?
abort_with("Set 'tool_name' inside tool.yml") if tool_name.blank?
abort_with("Set 'description' inside tool.yml") if description.blank?
abort_with("Set 'summary' inside tool.yml") if summary.blank?

parameters =
  Array(config["parameters"]).map do |param|
    {
      name: param["name"].to_s.strip,
      type: param["type"].to_s.strip.presence || "string",
      description: param["description"].to_s.strip,
      required: !!param["required"],
      enum: Array(param["enum"]).map(&:to_s).reject(&:blank?),
    }.tap do |hash|
      hash.delete(:enum) if hash[:enum].blank?
    end
  end

rag_cfg = config["rag"] || {}

tool_attrs = {
  name: name,
  tool_name: tool_name,
  description: description,
  summary: summary,
  parameters: parameters,
  script: script_source,
}
tool_attrs[:rag_chunk_tokens] = rag_cfg["chunk_tokens"].to_i if rag_cfg["chunk_tokens"].present?
tool_attrs[:rag_chunk_overlap_tokens] = rag_cfg["chunk_overlap_tokens"].to_i if rag_cfg["chunk_overlap_tokens"].present?
if rag_cfg.key?("llm_model_id") && !rag_cfg["llm_model_id"].nil?
  tool_attrs[:rag_llm_model_id] = rag_cfg["llm_model_id"].to_i
end

creator = User.where(admin: true).order(:id).first || Discourse.system_user
abort_with("Could not find an admin user or the Discourse system user") unless creator

llm_record = LlmModel.order(:id).first
abort_with("Configure at least one LLM model under Admin → Plugins → Discourse AI → LLMs") unless llm_record

tool = AiTool.new(tool_attrs.merge(created_by: creator))

unless tool.valid?
  abort_with("tool.yml validation errors:\n- #{tool.errors.full_messages.join("\n- ")}")
end

runner = tool.runner(payload_data, llm: llm_record.to_llm, bot_user: creator)
begin
  result = runner.invoke
rescue => e
  abort_with("Invocation failed: #{e.message}")
end

output = { "output" => result }
output["custom_raw"] = runner.custom_raw if runner.custom_raw.present?

puts JSON.pretty_generate(output)

details = runner.details rescue nil
if details.present?
  puts
  puts "details():"
  puts details
end
